{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User–Project–Quarter Panel (Refactored)\n",
        "\n",
        "Refactored pipeline to construct a user×project×quarter panel with:\n",
        "- commit-level collapse\n",
        "- counts and AI means\n",
        "- library/pair/combo novelty metrics\n",
        "- community enrichments (PMI‑Louvain, RCA‑SBM)\n",
        "- cushion and year subset\n",
        "\n",
        "Use `run_pipeline_in_notebook(...)` at the end to execute on your CSVs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Set, Tuple, Dict, Any\n",
        "import ast\n",
        "import itertools as it\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Settings\n",
        "# ---------------------------------------\n",
        "@dataclass(frozen=True)\n",
        "class Settings:\n",
        "    cushion_quarters: int = 3               # drop first K quarters per user\n",
        "    global_year_min: Optional[int] = 2020   # keep rows with year > this value; None=skip\n",
        "    top_k_libs: Optional[int] = 1000        # None => choose by coverage\n",
        "    top_k_coverage: float = 0.80            # used only if top_k_libs is None\n",
        "    min_combo_size: int = 2                 # commit combo size threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Utilities\n",
        "# ---------------------------------------\n",
        "def _safe_literal_eval(x: Any) -> List[Any]:\n",
        "    \"\"\"Safely parse a stringified Python literal into a Python object. Returns [].\"\"\"\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return []\n",
        "    if isinstance(x, (list, tuple, set)):\n",
        "        return list(x)\n",
        "    s = str(x).strip()\n",
        "    if not s or s == '[]':\n",
        "        return []\n",
        "    try:\n",
        "        val = ast.literal_eval(s)\n",
        "        if isinstance(val, (list, tuple, set)):\n",
        "            return list(val)\n",
        "        return [val]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "def _standardize_yq(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Normalize 'year_quarter' variants to 'YYYYQx'.\"\"\"\n",
        "    def _norm(yq: Any) -> str:\n",
        "        if yq is None or (isinstance(yq, float) and np.isnan(yq)):\n",
        "            return ''\n",
        "        s = str(yq).strip().upper().replace('-', '')\n",
        "        year = ''.join(ch for ch in s if ch.isdigit())[:4]\n",
        "        qpos = s.find('Q')\n",
        "        if year and qpos != -1 and qpos + 1 < len(s) and s[qpos+1].isdigit():\n",
        "            return f\"{year}Q{s[qpos+1]}\"\n",
        "        last_digit = next((ch for ch in reversed(s) if ch.isdigit()), '')\n",
        "        return f\"{year}Q{last_digit}\" if year and last_digit else s\n",
        "\n",
        "    return series.astype(str).map(_norm)\n",
        "\n",
        "\n",
        "def _quarter_order_map(yq: pd.Series) -> Dict[str, int]:\n",
        "    uniq = pd.Index(sorted(pd.Index(yq.unique())))\n",
        "    return {v: i for i, v in enumerate(uniq)}\n",
        "\n",
        "\n",
        "def _ordered_by_quarter(df: pd.DataFrame, yq_col: str = 'year_quarter') -> pd.DataFrame:\n",
        "    order = _quarter_order_map(df[yq_col])\n",
        "    return df.assign(_ord=df[yq_col].map(order)).sort_values(['_ord'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Commit-level preparation\n",
        "# ---------------------------------------\n",
        "def build_commit_level(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Collapse function/file-level rows to one row per commit_id.\"\"\"\n",
        "    required = ['user_hashed','project','year_quarter','commit_id','file_name','imports_commit']\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing columns in raw df: {missing}\")\n",
        "\n",
        "    df = df.copy()\n",
        "    df['year_quarter'] = _standardize_yq(df['year_quarter'])\n",
        "    df['_libs_row'] = df['imports_commit'].apply(_safe_literal_eval)\n",
        "\n",
        "    commit_file = df[['user_hashed','project','year_quarter','commit_id','file_name']].drop_duplicates()\n",
        "    files_per_commit = (\n",
        "        commit_file\n",
        "        .groupby(['user_hashed','project','year_quarter','commit_id'], as_index=False)\n",
        "        .agg(n_files=('file_name', 'nunique'))\n",
        "    )\n",
        "\n",
        "    libs_per_commit = (\n",
        "        df.groupby(['user_hashed','project','year_quarter','commit_id'], as_index=False)\n",
        "          .agg(libs=('_libs_row', lambda L: sorted(set(str(z).strip() for sub in L for z in (sub if isinstance(sub, (list, tuple, set)) else [sub]) if str(z).strip()))))\n",
        "    )\n",
        "    libs_per_commit['import_commit'] = libs_per_commit['libs'].apply(lambda L: int(len(L) > 0))\n",
        "\n",
        "    commit_level = files_per_commit.merge(libs_per_commit, on=['user_hashed','project','year_quarter','commit_id'], how='outer')\n",
        "    return commit_level[['commit_id','user_hashed','project','year_quarter','n_files','import_commit','libs']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Base panel\n",
        "# ---------------------------------------\n",
        "def build_base_panel(df: pd.DataFrame, commit_level: pd.DataFrame) -> pd.DataFrame:\n",
        "    # counts per (u,p,yq)\n",
        "    counts = (\n",
        "        commit_level\n",
        "        .groupby(['user_hashed','project','year_quarter'], as_index=False)\n",
        "        .agg(\n",
        "            n_commits=('commit_id','nunique'),\n",
        "            n_commits_multi_files=('n_files', lambda s: int((s > 1).sum())),\n",
        "            n_commits_with_import=('import_commit','sum')\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # user-quarter AI means\n",
        "    ai_cols = [c for c in ('ai_share','ai_share_window') if c in df.columns]\n",
        "    ai_agg = {}\n",
        "    if 'ai_share' in ai_cols:\n",
        "        ai_agg['mean_ai_share'] = ('ai_share','mean')\n",
        "    if 'ai_share_window' in ai_cols:\n",
        "        ai_agg['ai_share_window'] = ('ai_share_window','mean')\n",
        "    ai_user_yq = df.groupby(['user_hashed','year_quarter'], as_index=False).agg(**ai_agg) if ai_agg else pd.DataFrame(columns=['user_hashed','year_quarter'])\n",
        "\n",
        "    # full grid\n",
        "    user_project_pairs = df[['user_hashed','project']].drop_duplicates()\n",
        "    all_yq = pd.DataFrame({'year_quarter': np.sort(df['year_quarter'].unique())})\n",
        "    full_grid = user_project_pairs.assign(_k=1).merge(all_yq.assign(_k=1), on='_k', how='outer').drop(columns=['_k'])\n",
        "\n",
        "    panel = full_grid.merge(counts, on=['user_hashed','project','year_quarter'], how='left')                      .merge(ai_user_yq, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    panel[['n_commits','n_commits_multi_files','n_commits_with_import']] = (\n",
        "        panel[['n_commits','n_commits_multi_files','n_commits_with_import']].fillna(0).astype('int64')\n",
        "    )\n",
        "\n",
        "    # project size summaries\n",
        "    project_sizes = (\n",
        "        df.groupby('project', as_index=False)\n",
        "          .agg(\n",
        "              project_n_people=('user_hashed','nunique'),\n",
        "              project_n_commits=('commit_id','nunique'),\n",
        "              project_n_files=('file_name','nunique')\n",
        "          )\n",
        "    )\n",
        "    panel = panel.merge(project_sizes, on='project', how='left')\n",
        "\n",
        "    # restrict to between first & last active quarter for each (user, project)\n",
        "    order = _quarter_order_map(df['year_quarter'])\n",
        "    panel['_ord'] = panel['year_quarter'].map(order)\n",
        "    bounds = (\n",
        "        panel.loc[panel['n_commits'] > 0, ['user_hashed','project','_ord']]\n",
        "             .groupby(['user_hashed','project'], as_index=False)\n",
        "             .agg(first_ord=('_ord','min'), last_ord=('_ord','max'))\n",
        "    )\n",
        "    panel = panel.merge(bounds, on=['user_hashed','project'], how='left')\n",
        "    panel = panel[(panel['_ord'] >= panel['first_ord']) & (panel['_ord'] <= panel['last_ord'])]                 .drop(columns=['_ord','first_ord','last_ord'])                 .reset_index(drop=True)\n",
        "\n",
        "    panel['year'] = panel['year_quarter'].str.slice(0,4).astype('int64')\n",
        "    panel['quarter'] = panel['year_quarter'].str.slice(-1)\n",
        "    return panel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Novelty helpers\n",
        "# ---------------------------------------\n",
        "def _per_user_chronological_counts(df: pd.DataFrame, list_col: str, user_col: str='user_hashed', yq_col: str='year_quarter'):\n",
        "    uniq_counts: List[int] = []\n",
        "    new_counts: List[int] = []\n",
        "    for _, g in df.groupby(user_col, sort=False):\n",
        "        seen: Set[Any] = set()\n",
        "        for items in g[list_col]:\n",
        "            cur = set(items)\n",
        "            uniq_counts.append(len(cur))\n",
        "            new_counts.append(len(cur - seen))\n",
        "            seen |= cur\n",
        "    return uniq_counts, new_counts\n",
        "\n",
        "\n",
        "def _global_novelty_over_time(df: pd.DataFrame, list_col: str, yq_col: str='year_quarter', order_map: Optional[Dict[str,int]] = None) -> List[int]:\n",
        "    if order_map is None:\n",
        "        order_map = _quarter_order_map(df[yq_col])\n",
        "    tmp = df.assign(_ord=df[yq_col].map(order_map)).sort_values(['_ord','user_hashed'])\n",
        "    seen: Set[Any] = set()\n",
        "    out: List[int] = []\n",
        "    for items in tmp[list_col]:\n",
        "        cur = set(items)\n",
        "        out.append(len(cur - seen))\n",
        "        seen |= cur\n",
        "    tmp = tmp.assign(_glob=out)\n",
        "    tmp = tmp[['user_hashed','year_quarter','_glob']].reset_index(drop=True)\n",
        "    return df.merge(tmp, on=['user_hashed','year_quarter'], how='left')['_glob'].tolist()\n",
        "\n",
        "\n",
        "def commit_libs(df_commit: pd.DataFrame) -> pd.DataFrame:\n",
        "    cols = ['commit_id','user_hashed','year_quarter','libs']\n",
        "    missing = [c for c in cols if c not in df_commit.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing columns in commit_level for commit_libs: {missing}\")\n",
        "    return df_commit[cols].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Library metrics (user×quarter)\n",
        "# ---------------------------------------\n",
        "def build_user_quarter_library_metrics(panel: pd.DataFrame, df_commit: pd.DataFrame) -> pd.DataFrame:\n",
        "    cl = commit_libs(df_commit)\n",
        "    uq = (\n",
        "        cl.groupby(['user_hashed','year_quarter'], as_index=False)\n",
        "          .agg(lib_lists=('libs', list))\n",
        "          .sort_values(['user_hashed','year_quarter'])\n",
        "    )\n",
        "    uq['libs_flat'] = uq['lib_lists'].apply(lambda lists: [lib for row in lists for lib in row])\n",
        "\n",
        "    uniq, new = _per_user_chronological_counts(uq, 'libs_flat')\n",
        "    uq['unique_libs'] = uniq\n",
        "    uq['new_unique_libs'] = new\n",
        "    uq['globally_new_libs'] = _global_novelty_over_time(uq, 'libs_flat')\n",
        "\n",
        "    grid = panel[['user_hashed','year_quarter']].drop_duplicates()\n",
        "    out = grid.merge(uq[['user_hashed','year_quarter','unique_libs','new_unique_libs','globally_new_libs']],\n",
        "                     on=['user_hashed','year_quarter'], how='left')\n",
        "    out[['unique_libs','new_unique_libs','globally_new_libs']] = out[['unique_libs','new_unique_libs','globally_new_libs']].fillna(0).astype('int64')\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Pair metrics (top‑K libraries)\n",
        "# ---------------------------------------\n",
        "def build_user_quarter_pair_metrics(panel: pd.DataFrame, df_commit: pd.DataFrame, top_k_libs: Optional[int]=None, top_k_coverage: float=0.80) -> pd.DataFrame:\n",
        "    cl = commit_libs(df_commit)\n",
        "    lib_freq = pd.Series((lib for libs in cl['libs'] for lib in libs)).value_counts()\n",
        "    if lib_freq.empty:\n",
        "        k_libs: Set[str] = set()\n",
        "    else:\n",
        "        if top_k_libs is None:\n",
        "            cum = lib_freq.cumsum() / lib_freq.sum()\n",
        "            k = int((cum <= top_k_coverage).sum())\n",
        "            k = max(k, 1)\n",
        "        else:\n",
        "            k = int(top_k_libs)\n",
        "        k_libs = set(lib_freq.index[:k].tolist())\n",
        "\n",
        "    cl = cl.assign(libs_topk=cl['libs'].apply(lambda L: sorted(set(l for l in L if l in k_libs))))\n",
        "    cl['pairs'] = cl['libs_topk'].apply(lambda s: [tuple(sorted(p)) for p in it.combinations(s, 2)] if len(s) >= 2 else [])\n",
        "\n",
        "    uq = (\n",
        "        cl.groupby(['user_hashed','year_quarter'], as_index=False)\n",
        "          .agg(pair_lists=('pairs', list))\n",
        "          .sort_values(['user_hashed','year_quarter'])\n",
        "    )\n",
        "    uq['pairs_flat'] = uq['pair_lists'].apply(lambda lists: [p for row in lists for p in row])\n",
        "\n",
        "    uniq, new = _per_user_chronological_counts(uq, 'pairs_flat')\n",
        "    uq['unique_pairs_topk'] = uniq\n",
        "    uq['new_unique_pairs_topk'] = new\n",
        "    uq['globally_new_pairs_topk'] = _global_novelty_over_time(uq, 'pairs_flat')\n",
        "\n",
        "    grid = panel[['user_hashed','year_quarter']].drop_duplicates()\n",
        "    out = grid.merge(uq[['user_hashed','year_quarter','unique_pairs_topk','new_unique_pairs_topk','globally_new_pairs_topk']],\n",
        "                     on=['user_hashed','year_quarter'], how='left')\n",
        "    cols = ['unique_pairs_topk','new_unique_pairs_topk','globally_new_pairs_topk']\n",
        "    out[cols] = out[cols].fillna(0).astype('int64')\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Commit combo metrics (≥ min size)\n",
        "# ---------------------------------------\n",
        "def build_user_quarter_commit_combo_metrics(panel: pd.DataFrame, df_commit: pd.DataFrame, min_combo_size: int=2) -> pd.DataFrame:\n",
        "    cl = commit_libs(df_commit)\n",
        "    cl['combo'] = cl['libs'].apply(lambda s: tuple(sorted(set(s))) if len(set(s)) >= min_combo_size else None)\n",
        "    cl = cl[cl['combo'].notna()].copy()\n",
        "\n",
        "    uq = (\n",
        "        cl.groupby(['user_hashed','year_quarter'], as_index=False)\n",
        "          .agg(combo_list=('combo', list))\n",
        "          .sort_values(['user_hashed','year_quarter'])\n",
        "    )\n",
        "\n",
        "    uniq, new = _per_user_chronological_counts(uq, 'combo_list')\n",
        "    uq['unique_commit_combos'] = uniq\n",
        "    uq['new_unique_commit_combos'] = new\n",
        "    uq['globally_new_commit_combos'] = _global_novelty_over_time(uq, 'combo_list')\n",
        "\n",
        "    grid = panel[['user_hashed','year_quarter']].drop_duplicates()\n",
        "    out = grid.merge(uq[['user_hashed','year_quarter','unique_commit_combos','new_unique_commit_combos','globally_new_commit_combos']],\n",
        "                     on=['user_hashed','year_quarter'], how='left')\n",
        "    cols = ['unique_commit_combos','new_unique_commit_combos','globally_new_commit_combos']\n",
        "    out[cols] = out[cols].fillna(0).astype('int64')\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Community enrichment\n",
        "# ---------------------------------------\n",
        "def _build_library_to_community_map(frame: pd.DataFrame, list_col: str) -> Dict[str, Any]:\n",
        "    m = frame[['community', list_col]].copy()\n",
        "    m[list_col] = m[list_col].apply(_safe_literal_eval)\n",
        "    m = m.explode(list_col, ignore_index=True)\n",
        "    m['library'] = m[list_col].astype(str).str.strip()\n",
        "    m = m.dropna(subset=['library']).drop_duplicates('library', keep='first')\n",
        "    return dict(m[['library','community']].values)\n",
        "\n",
        "\n",
        "def _build_project_to_community(frame: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = frame[['community','description_simple','projects']].copy()\n",
        "    m['projects'] = m['projects'].apply(_safe_literal_eval)\n",
        "    m = m.explode('projects', ignore_index=True)\n",
        "    m = m.rename(columns={'projects':'project'})\n",
        "    m['project'] = m['project'].astype(str).str.strip()\n",
        "    m = m.drop_duplicates(subset='project', keep='first')\n",
        "    return m[['project','community','description_simple']].copy()\n",
        "\n",
        "\n",
        "def attach_project_communities(panel: pd.DataFrame,\n",
        "                               cluster_pmi_louv: Optional[pd.DataFrame] = None,\n",
        "                               cluster_rca_sbm: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
        "    out = panel.copy()\n",
        "    if cluster_pmi_louv is not None:\n",
        "        comm = _build_project_to_community(cluster_pmi_louv)\n",
        "        out = out.merge(comm, on='project', how='left')\n",
        "        out = out.rename(columns={'community':'community_pmi_louv','description_simple':'description_pmi_louv'})\n",
        "    if cluster_rca_sbm is not None:\n",
        "        comm = _build_project_to_community(cluster_rca_sbm)\n",
        "        out = out.merge(comm, on='project', how='left')\n",
        "        out = out.rename(columns={'community':'community_rca_sbm','description_simple':'description_rca_sbm'})\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_user_quarter_community_metrics(panel: pd.DataFrame, df_commit: pd.DataFrame, lib_to_comm: Dict[str, Any], suffix: str) -> pd.DataFrame:\n",
        "    cl = commit_libs(df_commit)\n",
        "    cl['commits_communities'] = cl['libs'].apply(lambda L: sorted(set(lib_to_comm.get(lib) for lib in L if lib in lib_to_comm)))\n",
        "    uq = (\n",
        "        cl.groupby(['user_hashed','year_quarter'], as_index=False)\n",
        "          .agg(comm_lists=('commits_communities', list))\n",
        "          .sort_values(['user_hashed','year_quarter'])\n",
        "    )\n",
        "    uq['comm_flat'] = uq['comm_lists'].apply(lambda lists: [c for row in lists for c in row])\n",
        "\n",
        "    uniq, new = _per_user_chronological_counts(uq, 'comm_flat')\n",
        "    uq[f'unique_import_communities_{suffix}'] = uniq\n",
        "    uq[f'new_unique_import_communities_{suffix}'] = new\n",
        "    uq[f'globally_new_import_communities_{suffix}'] = _global_novelty_over_time(uq, 'comm_flat')\n",
        "\n",
        "    grid = panel[['user_hashed','year_quarter']].drop_duplicates()\n",
        "    out = grid.merge(\n",
        "        uq[['user_hashed','year_quarter',\n",
        "            f'unique_import_communities_{suffix}',\n",
        "            f'new_unique_import_communities_{suffix}',\n",
        "            f'globally_new_import_communities_{suffix}']],\n",
        "        on=['user_hashed','year_quarter'], how='left'\n",
        "    )\n",
        "    cols = [f'unique_import_communities_{suffix}',\n",
        "            f'new_unique_import_communities_{suffix}',\n",
        "            f'globally_new_import_communities_{suffix}']\n",
        "    out[cols] = out[cols].fillna(0).astype('int64')\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Cushion & year subset\n",
        "# ---------------------------------------\n",
        "def apply_cushion(panel: pd.DataFrame, cushion: int) -> pd.DataFrame:\n",
        "    order = _quarter_order_map(panel['year_quarter'])\n",
        "    out = panel.assign(_ord=panel['year_quarter'].map(order)).sort_values(['user_hashed','_ord'])\n",
        "    def _drop_k(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        return g.iloc[cushion:]\n",
        "    out = out.groupby('user_hashed', group_keys=False).apply(_drop_k).reset_index(drop=True)\n",
        "    return out.drop(columns=['_ord'])\n",
        "\n",
        "\n",
        "def global_subset_year(panel: pd.DataFrame, year_min_exclusive: int) -> pd.DataFrame:\n",
        "    if 'year' not in panel.columns:\n",
        "        panel = panel.assign(year=panel['year_quarter'].str.slice(0,4).astype('int64'))\n",
        "    return panel[panel['year'] > year_min_exclusive].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Orchestrator\n",
        "# ---------------------------------------\n",
        "def build_full_panel(df_raw: pd.DataFrame,\n",
        "                     cluster_pmi_louv: Optional[pd.DataFrame] = None,\n",
        "                     cluster_rca_sbm: Optional[pd.DataFrame] = None,\n",
        "                     settings: Settings = Settings()) -> pd.DataFrame:\n",
        "    commits = build_commit_level(df_raw)\n",
        "    panel = build_base_panel(df_raw, commits)\n",
        "\n",
        "    lib_metrics = build_user_quarter_library_metrics(panel, commits)\n",
        "    panel = panel.merge(lib_metrics, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    pair_metrics = build_user_quarter_pair_metrics(panel, commits, settings.top_k_libs, settings.top_k_coverage)\n",
        "    panel = panel.merge(pair_metrics, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    combo_metrics = build_user_quarter_commit_combo_metrics(panel, commits, settings.min_combo_size)\n",
        "    panel = panel.merge(combo_metrics, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    panel = attach_project_communities(panel, cluster_pmi_louv, cluster_rca_sbm)\n",
        "\n",
        "    if cluster_pmi_louv is not None and 'libraries' in cluster_pmi_louv.columns:\n",
        "        lib2comm_pmi = _build_library_to_community_map(cluster_pmi_louv, 'libraries')\n",
        "        uqc_pmi = build_user_quarter_community_metrics(panel, commits, lib2comm_pmi, 'pmi_louv')\n",
        "        panel = panel.merge(uqc_pmi, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    if cluster_rca_sbm is not None and 'nodes' in cluster_rca_sbm.columns:\n",
        "        lib2comm_rca = _build_library_to_community_map(cluster_rca_sbm, 'nodes')\n",
        "        uqc_rca = build_user_quarter_community_metrics(panel, commits, lib2comm_rca, 'rca_sbm')\n",
        "        panel = panel.merge(uqc_rca, on=['user_hashed','year_quarter'], how='left')\n",
        "\n",
        "    if settings.cushion_quarters and settings.cushion_quarters > 0:\n",
        "        panel = apply_cushion(panel, settings.cushion_quarters)\n",
        "\n",
        "    if settings.global_year_min is not None:\n",
        "        panel = global_subset_year(panel, settings.global_year_min)\n",
        "\n",
        "    # Back-compat column aliases\n",
        "    alias_map = {\n",
        "        'unique_import_entries': 'unique_libs',\n",
        "        'new_unique_import_entries': 'new_unique_libs',\n",
        "        'globally_new_unique_import_entries': 'globally_new_libs',\n",
        "        'unique_import_lists': 'unique_commit_combos',\n",
        "        'new_unique_import_lists': 'new_unique_commit_combos',\n",
        "        'globally_new_unique_import_lists': 'globally_new_commit_combos',\n",
        "    }\n",
        "    for alias, src_col in alias_map.items():\n",
        "        if src_col in panel.columns and alias not in panel.columns:\n",
        "            panel[alias] = panel[src_col]\n",
        "\n",
        "    # Final column order (only include if present)\n",
        "    final_cols = [\n",
        "        'user_hashed','project','year_quarter','year','quarter',\n",
        "        'n_commits','n_commits_multi_files','n_commits_with_import',\n",
        "        'mean_ai_share',#'ai_share_window',\n",
        "        'project_n_people','project_n_commits','project_n_files',\n",
        "        'unique_libs','new_unique_libs','globally_new_libs',\n",
        "        'unique_pairs_topk','new_unique_pairs_topk','globally_new_pairs_topk',\n",
        "        'unique_commit_combos','new_unique_commit_combos','globally_new_commit_combos',\n",
        "        'community_pmi_louv','description_pmi_louv',\n",
        "        'community_rca_sbm','description_rca_sbm',\n",
        "        'unique_import_communities_pmi_louv','new_unique_import_communities_pmi_louv','globally_new_import_communities_pmi_louv',\n",
        "        'unique_import_communities_rca_sbm','new_unique_import_communities_rca_sbm','globally_new_import_communities_rca_sbm',\n",
        "    ]\n",
        "    existing = [c for c in final_cols if c in panel.columns]\n",
        "    panel = panel[existing + [c for c in panel.columns if c not in existing]]\n",
        "    return panel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Notebook runner helper\n",
        "# ---------------------------------------\n",
        "def run_pipeline_in_notebook(raw_csv: str,\n",
        "                             out_csv: str = 'panel.csv',\n",
        "                             cluster_pmi_louv_csv: Optional[str] = None,\n",
        "                             cluster_rca_sbm_csv: Optional[str] = None,\n",
        "                             settings: Settings = Settings()) -> pd.DataFrame:\n",
        "    df = pd.read_csv(raw_csv)\n",
        "    cluster_pmi = pd.read_csv(cluster_pmi_louv_csv) if cluster_pmi_louv_csv else None\n",
        "    cluster_rca = pd.read_csv(cluster_rca_sbm_csv) if cluster_rca_sbm_csv else None\n",
        "    panel = build_full_panel(df, cluster_pmi, cluster_rca, settings=settings)\n",
        "    panel.to_csv(out_csv, index=False)\n",
        "    return panel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce7d6e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "run_pipeline_in_notebook(\"./final_data/raw_data_encrypted.csv.zip\",cluster_pmi_louv_csv=\"./final_data/project_by_task_from_library_uni_network.csv\",cluster_rca_sbm_csv=\"./final_data/project_by_project_library_bipartite_rca_sbm.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: quick smoke test (synthetic data)\n",
        "Run the cell below to verify shapes on toy data. Safe to skip.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic minimal example (uncomment to run)\n",
        "# data = [\n",
        "#     dict(user_hashed='u1', project='pA', commit_id='c1', file_name='f1.py', year_quarter='2021-Q1', imports_commit=\"['pandas','numpy']\", ai_share=0.3),\n",
        "#     dict(user_hashed='u1', project='pA', commit_id='c2', file_name='f2.py', year_quarter='2021-Q2', imports_commit=\"['pandas','scipy']\", ai_share=0.5),\n",
        "#     dict(user_hashed='u2', project='pB', commit_id='c3', file_name='g1.py', year_quarter='2021-Q2', imports_commit=\"['numpy']\", ai_share=0.4),\n",
        "# ]\n",
        "# df = pd.DataFrame(data)\n",
        "# panel = build_full_panel(df, settings=Settings(cushion_quarters=0, global_year_min=None, top_k_libs=2, min_combo_size=2))\n",
        "# panel.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "github_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
