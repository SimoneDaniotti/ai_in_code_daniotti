{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f6ff49-2997-4b65-9a1a-ddf829ec6945",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba5a-dbc7-40f5-9b51-b228f1e90d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T10:43:53.967322Z",
     "iopub.status.busy": "2025-12-15T10:43:53.967032Z",
     "iopub.status.idle": "2025-12-15T10:44:07.505393Z",
     "shell.execute_reply": "2025-12-15T10:44:07.504751Z",
     "shell.execute_reply.started": "2025-12-15T10:43:53.967290Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import pickle5 as pickle\n",
    "import pickle\n",
    "def save_obj(obj, name, data_path_save = 'obj/'):\n",
    "    with open(data_path_save + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_obj(name, data_path_load = 'obj/'):\n",
    "    with open(data_path_load + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "data_path = '../final_data/s8_salary_data/'\n",
    "df_task_statement = pd.read_excel(data_path + 'Task Statements.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "## task statements\n",
    "from collections import defaultdict\n",
    "task_content = {}\n",
    "job_tasklist = defaultdict(list)\n",
    "job_content = {}\n",
    "job_tasktype_list = defaultdict(list)\n",
    "for soc_id,soc_job, task_id, task, core_sup in zip(df_task_statement['O*NET-SOC Code'],df_task_statement['Title'], df_task_statement['Task ID'],  df_task_statement['Task'], df_task_statement['Task Type']):\n",
    "    task_content[str(task_id)] = task\n",
    "    job_tasklist[soc_id].append(str(task_id))\n",
    "    job_content[soc_id] = soc_job\n",
    "    if pd.isna(core_sup):\n",
    "        job_tasktype_list[soc_id].append('Not Available')\n",
    "    else:\n",
    "        job_tasktype_list[soc_id].append(core_sup)\n",
    "\n",
    "\n",
    "## task rating\n",
    "df_task_rating = pd.read_excel(data_path + 'Task Ratings.xlsx')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "category_dict = {str(i+1):i for i in range(7)}\n",
    "job_task_bool = defaultdict(bool)\n",
    "job_task_value_ft = {}\n",
    "job_task_value_im = defaultdict(float)\n",
    "job_task_value_rt = defaultdict(float)\n",
    "\n",
    "for soc_id, task_id, scale_id, category, data_value in zip(df_task_rating['O*NET-SOC Code'], df_task_rating['Task ID'], df_task_rating['Scale ID'], df_task_rating['Category'], df_task_rating['Data Value']):\n",
    "    if scale_id == 'FT':\n",
    "        if not job_task_bool[(soc_id, str(task_id))]:\n",
    "            job_task_bool[(soc_id, str(task_id))] = True\n",
    "            job_task_value_ft[(soc_id, str(task_id))] = [0 for _ in range(7)]\n",
    "\n",
    "        job_task_value_ft[(soc_id, str(task_id))][category_dict[str(int(category))]] = data_value\n",
    "\n",
    "    if scale_id == 'IM':\n",
    "        job_task_value_im[(soc_id, str(task_id))] = data_value\n",
    "\n",
    "    if scale_id == 'RT':\n",
    "        job_task_value_rt[(soc_id, str(task_id))] = data_value\n",
    "\n",
    "print(len(job_task_value_ft), len(job_task_value_im), len(job_task_value_rt))\n",
    "\n",
    "\n",
    "missing_pairs = []\n",
    "job_ftlist = defaultdict(list)\n",
    "job_tasklist_ft = defaultdict(list)\n",
    "\n",
    "for soc_id, tasks in job_tasklist.items():\n",
    "    for task_id in tasks:\n",
    "        if job_task_bool[(soc_id, task_id)]:\n",
    "            job_ftlist[soc_id].append(job_task_value_ft[(soc_id, task_id)])\n",
    "            job_tasklist_ft[soc_id].append(task_id)\n",
    "\n",
    "        else:\n",
    "            missing_pairs.append([(soc_id, task_id)])\n",
    "\n",
    "\n",
    "save_obj(job_ftlist,'job_ftvector_list', data_path)\n",
    "save_obj(job_tasklist_ft,'job_tasklist_ft', data_path)\n",
    "save_obj(missing_pairs, 'missing_pairs', data_path)\n",
    "save_obj(job_task_value_rt, 'job_task_value_rt', data_path)\n",
    "save_obj(job_task_value_im, 'job_task_value_im', data_path)\n",
    "save_obj(task_content, 'task_content',data_path)\n",
    "save_obj(job_content, 'job_content',data_path)\n",
    "save_obj(job_tasktype_list, 'job_tasktype_list',data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a260a24-bb9d-4089-9636-0a7a64c3e6bb",
   "metadata": {},
   "source": [
    "# LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228a091-b9b5-4b32-9678-2c772c1f38eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T10:44:07.506017Z",
     "iopub.status.busy": "2025-12-15T10:44:07.505852Z",
     "iopub.status.idle": "2025-12-15T10:44:07.509074Z",
     "shell.execute_reply": "2025-12-15T10:44:07.508670Z",
     "shell.execute_reply.started": "2025-12-15T10:44:07.506004Z"
    }
   },
   "outputs": [],
   "source": [
    "# job_tasklist_ft = load_obj('job_tasklist_ft', data_path)\n",
    "# task_content =load_obj('task_content',data_path)\n",
    "# job_content = load_obj('job_content',data_path)\n",
    "\n",
    "# import requests\n",
    "# from collections import defaultdict\n",
    "\n",
    "# job_task_programming_score = {}\n",
    "\n",
    "# # Ollama API 地址\n",
    "# api_url = \"\"\n",
    "\n",
    "# for soc_id, task_list in job_tasklist_ft.items():\n",
    "#     print(job_content[soc_id])\n",
    "#     job_task_programming_score[soc_id] = {}\n",
    "    \n",
    "#     for t in task_list:\n",
    "#         question = ''''''\n",
    "        \n",
    "#         payload = {\n",
    "#             \"model\": \"llama3.3:latest\",  # 模型名称\n",
    "#             \"prompt\":question,  # 输入的提示文本\n",
    "#             \"stream\": False,  # 是否流式输出\n",
    "#             \"max_tokens\": 20000,  # 限制生成的最大长度\n",
    "#             \"temperature\": 0.1,  # 控制生成文本的随机性\n",
    "#             \"top_p\": 0.1,  # 控制生成文本的多样性\n",
    "#         }\n",
    "        \n",
    "#         # 发送 POST 请求\n",
    "#         response = requests.post(api_url, json=payload)\n",
    "        \n",
    "#         # 检查响应状态\n",
    "#         if response.status_code == 200:\n",
    "#             # 解析响应内容\n",
    "#             result = response.json()\n",
    "#             result_temp = result.get(\"response\")\n",
    "#             print(\"生成的文本：\", result_temp)\n",
    "#             job_task_programming_score[soc_id][t] = result_temp\n",
    "#         else:\n",
    "#             print(\"请求失败，状态码：\", response.status_code)\n",
    "#             print(\"错误信息：\", response.text)\n",
    "#             job_task_programming_score[soc_id][t] = 10086\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aac5fc-0b57-47a1-a34f-f2b9379ac265",
   "metadata": {},
   "source": [
    "# get working hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee6af2-0b7a-42af-93fb-400f5a4e5730",
   "metadata": {},
   "source": [
    "## examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f6f91-6b7f-4a0e-acbb-7002ecd553a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T10:44:07.509488Z",
     "iopub.status.busy": "2025-12-15T10:44:07.509384Z",
     "iopub.status.idle": "2025-12-15T10:44:07.578257Z",
     "shell.execute_reply": "2025-12-15T10:44:07.577601Z",
     "shell.execute_reply.started": "2025-12-15T10:44:07.509478Z"
    }
   },
   "outputs": [],
   "source": [
    "job_ftlist = load_obj('job_ftvector_list', data_path)\n",
    "job_tasklist_ft = load_obj('job_tasklist_ft', data_path)\n",
    "\n",
    "import numpy as np\n",
    "hour_vector = [0, 0.02*8, 0.05*8, 0.08*8, 0.10*8, 0.25*8, 0.5*8]\n",
    "\n",
    "soc_working_time_vector_per_day = {}\n",
    "for soc_id, ft_matrix in job_ftlist.items():\n",
    "    ftm = np.array(ft_matrix)\n",
    "    ftm_normalized = ftm / (np.sum(ftm, axis = 0) + 0.0000000001)\n",
    "    soc_working_time_vector_per_day[soc_id] = ftm_normalized.dot(np.array(hour_vector).T) / 8\n",
    "\n",
    "save_obj(soc_working_time_vector_per_day, 'soc_working_time_vector_example_1_ft_normalized', data_path)\n",
    "\n",
    "\n",
    "job_ftlist = load_obj('job_ftvector_list', data_path)\n",
    "job_tasklist_ft = load_obj('job_tasklist_ft', data_path)\n",
    "\n",
    "import numpy as np\n",
    "hour_vector = [0.5, 1, 4, 48, 240, 480, 1920]\n",
    "\n",
    "soc_working_time_vector = {}\n",
    "for soc_id, ft_matrix in job_ftlist.items():\n",
    "    temp = np.array(ft_matrix).dot(np.array(hour_vector).T)\n",
    "    soc_working_time_vector[soc_id] = temp / np.sum(temp)\n",
    "\n",
    "\n",
    "save_obj(soc_working_time_vector, 'soc_working_time_vector_example_2_hour_vector', data_path)\n",
    "\n",
    "\n",
    "job_im_dict = load_obj('job_task_value_im', data_path)\n",
    "job_tasklist = load_obj('job_tasklist_ft', data_path)\n",
    "\n",
    "job_im_weighted_time = {}\n",
    "job_imrank_weighted_time = {}\n",
    "\n",
    "for soc_id, tasklist in job_tasklist.items():\n",
    "    im_temp = np.array([job_im_dict[(soc_id, t)] for t in tasklist])\n",
    "    arr = -im_temp\n",
    "    temp = arr.argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(arr)) + 1\n",
    "    rank_weight = 2 * len(tasklist) - ranks\n",
    "\n",
    "    job_im_weighted_time[soc_id] = im_temp / np.sum(im_temp)\n",
    "    job_imrank_weighted_time[soc_id] = rank_weight / np.sum(rank_weight)\n",
    "\n",
    "save_obj(job_im_weighted_time, 'job_im_weighted_time', data_path)\n",
    "save_obj(job_imrank_weighted_time, 'job_imrank_weighted_time', data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f8f63-d63c-490f-9e42-bca99fcb8e78",
   "metadata": {},
   "source": [
    "## BLS hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf87a63-8e33-44e4-b0d3-f7eb2c065a47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T10:44:49.626099Z",
     "iopub.status.busy": "2025-12-15T10:44:49.625381Z",
     "iopub.status.idle": "2025-12-15T10:44:49.954397Z",
     "shell.execute_reply": "2025-12-15T10:44:49.953745Z",
     "shell.execute_reply.started": "2025-12-15T10:44:49.626070Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_job_salary = pd.read_excel(data_path + 'national_M2024_dl_pure_data.xlsx')\n",
    "df_job_salary = df_job_salary[df_job_salary['O_GROUP'] == 'detailed']\n",
    "\n",
    "annual_salary_bls = {}\n",
    "hour_salary_bls = {}\n",
    "\n",
    "employment_count_bls = {}\n",
    "\n",
    "for soc_id, h, a, emp in zip(df_job_salary['OCC_CODE'], df_job_salary['H_MEAN'], df_job_salary['A_MEAN'], df_job_salary['TOT_EMP']):\n",
    "\n",
    "    employment_count_bls[soc_id] = emp\n",
    "\n",
    "    if a != '*':\n",
    "        annual_salary_bls[soc_id] = a\n",
    "\n",
    "    if h != '*':\n",
    "        hour_salary_bls[soc_id] = h\n",
    "        \n",
    "hour_salary_bls_adjust = {}\n",
    "hour_salary_bls_adjust.update(hour_salary_bls)\n",
    "for soc_id, a in annual_salary_bls.items():\n",
    "    if soc_id not in hour_salary_bls:\n",
    "        hour_salary_bls_adjust[soc_id] = a / 2080\n",
    "\n",
    "annual_salary_bls_adjust = {}\n",
    "annual_salary_bls_adjust.update(annual_salary_bls)\n",
    "for soc_id, h in hour_salary_bls.items():\n",
    "    if soc_id not in annual_salary_bls:\n",
    "        annual_salary_bls_adjust[soc_id] = h * 2080\n",
    "\n",
    "for soc_id, h in hour_salary_bls_adjust.items():\n",
    "    if abs(h*2080-annual_salary_bls_adjust[soc_id])>20:\n",
    "        print(soc_id)\n",
    "\n",
    "save_obj(annual_salary_bls,'annual_salary_bls', data_path)\n",
    "save_obj(hour_salary_bls,'hour_salary_bls', data_path)\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "job_task_programming_score = load_obj(\"job_task_programming_score_gemini_0_5_adjusted\", data_path)\n",
    "ps_temp = [ts for j,tsl in job_task_programming_score.items() for t, ts in tsl.items()]\n",
    "print(Counter(ps_temp))\n",
    "\n",
    "ps_dict = {'0':0, '1':0.0055, '2':0.18, '3':0.38, '4':0.63, '5':0.88}\n",
    "\n",
    "def calculate_programming_score(soc_working_time_vector):\n",
    "\n",
    "    job_programming_score_vector = {}\n",
    "    job_programming_hour = {}\n",
    "\n",
    "    for soc_id, tasklist in job_tasklist_ft.items():\n",
    "        job_programming_score_vector[soc_id] = np.array([ps_dict[job_task_programming_score[soc_id][t]] for t in tasklist])\n",
    "        job_programming_hour[soc_id] = soc_working_time_vector[soc_id].dot(job_programming_score_vector[soc_id])\n",
    "\n",
    "\n",
    "    from collections import defaultdict\n",
    "    job_programming_hour_6digit_temp = defaultdict(list)\n",
    "    for soc_id, h in job_programming_hour.items():\n",
    "        job_programming_hour_6digit_temp[soc_id[:7]].append(h)\n",
    "\n",
    "    job_programming_hour_6digit = {}\n",
    "    for soc_id, hs in job_programming_hour_6digit_temp.items():\n",
    "        job_programming_hour_6digit[soc_id] = np.mean(hs)\n",
    "\n",
    "\n",
    "    job_programming_hour_6digit['13-1020'] = (job_programming_hour_6digit['13-1023'] + job_programming_hour_6digit['13-1021'] + job_programming_hour_6digit['13-1022'])/3\n",
    "    del job_programming_hour_6digit['13-1023']\n",
    "    del job_programming_hour_6digit['13-1021']\n",
    "    del job_programming_hour_6digit['13-1022']\n",
    "\n",
    "    job_programming_hour_6digit['13-2020'] = job_programming_hour_6digit['13-2023']\n",
    "    del job_programming_hour_6digit['13-2023']\n",
    "\n",
    "    job_programming_hour_6digit['29-2010'] = (job_programming_hour_6digit['29-2011'] + job_programming_hour_6digit['29-2012'])/2\n",
    "    del job_programming_hour_6digit['29-2011']\n",
    "    del job_programming_hour_6digit['29-2012']\n",
    "\n",
    "\n",
    "    job_programming_hour_6digit['31-1120'] = (job_programming_hour_6digit['31-1121'] + job_programming_hour_6digit['31-1122'])/2\n",
    "    del job_programming_hour_6digit['31-1121']\n",
    "    del job_programming_hour_6digit['31-1122']\n",
    "\n",
    "    job_programming_hour_6digit['39-7010'] = (job_programming_hour_6digit['39-7011'] + job_programming_hour_6digit['39-7012'])/2\n",
    "    del job_programming_hour_6digit['39-7011']\n",
    "    del job_programming_hour_6digit['39-7012']\n",
    "\n",
    "    job_programming_hour_6digit['47-4090'] = (job_programming_hour_6digit['47-4091'] + job_programming_hour_6digit['47-4099'])/2\n",
    "    del job_programming_hour_6digit['47-4091']\n",
    "    del job_programming_hour_6digit['47-4099']\n",
    "\n",
    "    job_programming_hour_6digit['51-2020'] = (job_programming_hour_6digit['51-2021'] + job_programming_hour_6digit['51-2022'] + job_programming_hour_6digit['51-2023'])/3\n",
    "    del job_programming_hour_6digit['51-2021']\n",
    "    del job_programming_hour_6digit['51-2022']\n",
    "    del job_programming_hour_6digit['51-2023']\n",
    "    \n",
    "    job_programming_hour_6digit['51-2090'] = job_programming_hour_6digit['51-2092']\n",
    "    del job_programming_hour_6digit['51-2092']\n",
    "\n",
    "    return job_programming_hour_6digit\n",
    "\n",
    "\n",
    "soc_working_time_vector = load_obj('soc_working_time_vector_example_1_ft_normalized', data_path)\n",
    "job_programming_hour_6digit = calculate_programming_score(soc_working_time_vector)\n",
    "save_obj(job_programming_hour_6digit, 'job_programming_hour_6digit_BLS_example1', data_path)\n",
    "\n",
    "soc_working_time_vector = load_obj('soc_working_time_vector_example_2_hour_vector', data_path)\n",
    "job_programming_hour_6digit = calculate_programming_score(soc_working_time_vector)\n",
    "save_obj(job_programming_hour_6digit, 'job_programming_hour_6digit_BLS_example2', data_path)\n",
    "\n",
    "soc_working_time_vector = load_obj('job_im_weighted_time', data_path)\n",
    "job_programming_hour_6digit = calculate_programming_score(soc_working_time_vector)\n",
    "save_obj(job_programming_hour_6digit, 'job_programming_hour_6digit_BLS_im', data_path)\n",
    "\n",
    "soc_working_time_vector = load_obj('job_imrank_weighted_time', data_path)\n",
    "job_programming_hour_6digit = calculate_programming_score(soc_working_time_vector)\n",
    "save_obj(job_programming_hour_6digit, 'job_programming_hour_6digit_BLS_imrank', data_path)\n",
    "\n",
    "\n",
    "employment_count_bls['51-2020'] = 273300\n",
    "hour_salary_bls_adjust['51-2020'] = 22.15\n",
    "annual_salary_bls_adjust['51-2020'] = 46070\n",
    "\n",
    "save_obj(annual_salary_bls_adjust,'annual_salary_bls_adjust', data_path)\n",
    "save_obj(hour_salary_bls_adjust,'hour_salary_bls_adjust', data_path)\n",
    "save_obj(employment_count_bls,'employment_count_bls_adjust', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e59a02-cb7c-48ea-b6ce-10657dd2ea28",
   "metadata": {},
   "source": [
    "# Figure S8 - comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c7cb2-46c5-4dd3-a5e5-081f56b3474c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T10:49:32.974523Z",
     "iopub.status.busy": "2025-12-15T10:49:32.974172Z",
     "iopub.status.idle": "2025-12-15T10:50:07.931571Z",
     "shell.execute_reply": "2025-12-15T10:50:07.931162Z",
     "shell.execute_reply.started": "2025-12-15T10:49:32.974495Z"
    }
   },
   "outputs": [],
   "source": [
    "task_content = load_obj('task_content',data_path)\n",
    "job_content = load_obj('job_content',data_path)\n",
    "job_task_programming_score2 = load_obj(\"job_task_programming_score_gemini_0_5_adjusted\", data_path)\n",
    "job_task_programming_score1 = load_obj(\"job_task_programming_score_gemini_0_5\", data_path)\n",
    "job_task_programming_score3 = load_obj('job_task_programming_score_0_100_from_gemini', data_path)\n",
    "\n",
    "ps_dict1 = {'1':0, '2':0.125, '3':0.375, '4':0.625, '5':0.875}\n",
    "ps_dict2 = {'0':0, '1':0.055, '2':0.18, '3':0.38, '4':0.63, '5':0.88}\n",
    "ps_dict3 = {str(v):int(v)/100 for vv in job_task_programming_score3.values() for v in vv.values()}\n",
    "\n",
    "job_content_list = []\n",
    "task_content_list = []\n",
    "score1 = []\n",
    "percent1 = []\n",
    "score2 = []\n",
    "percent2 = []\n",
    "score3 = []\n",
    "percent3 = []\n",
    "for job, ps in job_task_programming_score1.items():\n",
    "    for t, s in ps.items():\n",
    "        if job in job_content and t in task_content:\n",
    "            job_content_list.append(job_content[job])\n",
    "            task_content_list.append(task_content[t])\n",
    "            score1.append(job_task_programming_score1[job][t])\n",
    "            percent1.append(ps_dict1[job_task_programming_score1[job][t]])\n",
    "            score2.append(job_task_programming_score2[job][t])\n",
    "            percent2.append(ps_dict2[job_task_programming_score2[job][t]])\n",
    "            score3.append(job_task_programming_score3[job][t])\n",
    "            percent3.append(ps_dict3[job_task_programming_score3[job][t]])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict({'job':job_content_list, 'task':task_content_list, 'score1':score1, 'percent1':percent1, 'score2':score2, 'percent2':percent2, 'score3':score3, 'percent3':percent3 })\n",
    "df.to_csv(data_path + 'task_score_percent.csv')\n",
    "\n",
    "job_ftlist = load_obj('job_ftvector_list', data_path)\n",
    "job_tasklist_ft = load_obj('job_tasklist_ft', data_path)\n",
    "\n",
    "job_task_programming_score1 = load_obj(\"job_task_programming_score_gemini_0_5\", data_path)\n",
    "ps_dict1 = {'1':0, '2':0.125, '3':0.375, '4':0.625, '5':0.875}\n",
    "\n",
    "\n",
    "job_task_programming_score2 = load_obj(\"job_task_programming_score_gemini_0_5_adjusted\", data_path)\n",
    "ps_dict2 = {'0':0, '1':0.055, '2':0.18, '3':0.38, '4':0.63, '5':0.88}\n",
    "\n",
    "\n",
    "job_task_programming_score3 = load_obj('job_task_programming_score_0_100_from_gemini', data_path)\n",
    "ps_dict3 = {str(v):int(v)/100 for vv in job_task_programming_score3.values() for v in vv.values()}\n",
    "\n",
    "\n",
    "df_skills = pd.read_excel(data_path + 'db_29_3_excel/Skills.xlsx')\n",
    "df_abilities = pd.read_excel(data_path + 'db_29_3_excel/Abilities.xlsx')\n",
    "df_knowledges = pd.read_excel(data_path + 'db_29_3_excel/Knowledge.xlsx')\n",
    "\n",
    "tasks_std = list(set(df_skills['Element ID'])) + list(set(df_abilities['Element ID'])) + list(set(df_knowledges['Element ID']))\n",
    "tasks_dict_std  = {s:i for i,s in enumerate(tasks_std)}\n",
    "\n",
    "import numpy as np\n",
    "job_list = list(set(df_skills['O*NET-SOC Code']))\n",
    "job_task_im_vector = {j:np.zeros(len(tasks_std)) for j in job_list}\n",
    "job_task_lv_vector = {j:np.zeros(len(tasks_std)) for j in job_list}\n",
    "\n",
    "for o, s, imlv, v in zip(df_skills['O*NET-SOC Code'], df_skills['Element ID'], df_skills['Scale ID'], df_skills['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "\n",
    "for o, s, imlv, v in zip(df_abilities['O*NET-SOC Code'], df_abilities['Element ID'], df_abilities['Scale ID'], df_abilities['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "\n",
    "for o, s, imlv, v in zip(df_knowledges['O*NET-SOC Code'], df_knowledges['Element ID'], df_knowledges['Scale ID'], df_knowledges['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "programming_id = '2.B.3.e'\n",
    "\n",
    "job_task_ps_im = {j: v[tasks_dict_std[programming_id]] / np.sum(v) for j,v in job_task_im_vector.items()}\n",
    "job_task_ps_lv = {j: v[tasks_dict_std[programming_id]] / np.sum(v) for j,v in job_task_lv_vector.items()}\n",
    "\n",
    "df_skills = pd.read_excel(data_path + 'db_29_3_excel/Skills.xlsx')\n",
    "df_abilities = pd.read_excel(data_path + 'db_29_3_excel/Abilities.xlsx')\n",
    "df_knowledges = pd.read_excel(data_path + 'db_29_3_excel/Knowledge.xlsx')\n",
    "\n",
    "tasks_std = list(set(df_skills['Element ID'])) + list(set(df_abilities['Element ID'])) + list(set(df_knowledges['Element ID']))\n",
    "tasks_dict_std  = {s:i for i,s in enumerate(tasks_std)}\n",
    "\n",
    "import numpy as np\n",
    "job_list = list(set(df_skills['O*NET-SOC Code']))\n",
    "job_task_im_vector = {j:np.zeros(len(tasks_std)) for j in job_list}\n",
    "job_task_lv_vector = {j:np.zeros(len(tasks_std)) for j in job_list}\n",
    "\n",
    "for o, s, imlv, v in zip(df_skills['O*NET-SOC Code'], df_skills['Element ID'], df_skills['Scale ID'], df_skills['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "\n",
    "for o, s, imlv, v in zip(df_abilities['O*NET-SOC Code'], df_abilities['Element ID'], df_abilities['Scale ID'], df_abilities['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "\n",
    "for o, s, imlv, v in zip(df_knowledges['O*NET-SOC Code'], df_knowledges['Element ID'], df_knowledges['Scale ID'], df_knowledges['Data Value']):\n",
    "    if imlv == 'IM':\n",
    "        job_task_im_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "    if imlv == 'LV':\n",
    "        job_task_lv_vector[o][tasks_dict_std[s]] = v\n",
    "\n",
    "programming_id = '2.B.3.e'\n",
    "\n",
    "job_task_ps_im = {j: v[tasks_dict_std[programming_id]] / np.sum(v) for j,v in job_task_im_vector.items()}\n",
    "job_task_ps_lv = {j: v[tasks_dict_std[programming_id]] / np.sum(v) for j,v in job_task_lv_vector.items()}\n",
    "\n",
    "\n",
    "\n",
    "soc_working_time_vector = load_obj('soc_working_time_vector_example_1_ft_normalized', data_path)\n",
    "\n",
    "job_programming_hour1 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour1[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict1[job_task_programming_score1[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "job_programming_hour2 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour2[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict2[job_task_programming_score2[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "job_programming_hour3 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour3[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict3[job_task_programming_score3[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "common_jobs = list(set(job_task_ps_im.keys()).intersection(job_programming_hour1.keys()))\n",
    "\n",
    "y_ex1 = [job_task_ps_im[j] for j in common_jobs]\n",
    "x1_ex1 = [job_programming_hour1[j] for j in common_jobs]\n",
    "x2_ex1 = [job_programming_hour2[j] for j in common_jobs]\n",
    "x3_ex1 = [job_programming_hour3[j] for j in common_jobs]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "soc_working_time_vector = load_obj('soc_working_time_vector_example_2_hour_vector', data_path)\n",
    "\n",
    "job_programming_hour1 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour1[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict1[job_task_programming_score1[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "job_programming_hour2 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour2[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict2[job_task_programming_score2[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "job_programming_hour3 = {}\n",
    "for soc_id, tasklist in job_tasklist_ft.items():\n",
    "    job_programming_hour3[soc_id] = soc_working_time_vector[soc_id].dot(np.array([ps_dict3[job_task_programming_score3[soc_id][t]] for t in tasklist]))\n",
    "\n",
    "common_jobs = list(set(job_task_ps_im.keys()).intersection(job_programming_hour1.keys()))\n",
    "\n",
    "y_ex2 = [job_task_ps_im[j] for j in common_jobs]\n",
    "x1_ex2 = [job_programming_hour1[j] for j in common_jobs]\n",
    "x2_ex2 = [job_programming_hour2[j] for j in common_jobs]\n",
    "x3_ex2 = [job_programming_hour3[j] for j in common_jobs]\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 设置 seaborn 的主题\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "xl = [x1_ex1,x2_ex1,x3_ex1,x1_ex2,x2_ex2,x3_ex2]\n",
    "yl = [y_ex1, y_ex1, y_ex1, y_ex2, y_ex2, y_ex2]\n",
    "x_labels = [\"programming share (prompt 1)\",\"programming share (prompt 2)\",\"programming share (prompt 3)\",\"programming share (prompt 1)\",\"programming share (prompt 2)\",\"programming share (prompt 3)\"]\n",
    "\n",
    "ccc = 0\n",
    "for x,y,l in zip(xl, yl, x_labels):\n",
    "    ccc += 1\n",
    "\n",
    "    # 1. 计算皮尔逊相关系数和 p-value\n",
    "    corr_coefficient, p_value = pearsonr(x, y)\n",
    "\n",
    "    # 2. 绘制散点图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    import matplotlib.font_manager\n",
    "\n",
    "    font_dirs = [f'{data_path}helvetica-255/', ]\n",
    "    font_files = matplotlib.font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "    for ff in font_files:\n",
    "        matplotlib.font_manager.fontManager.addfont(ff)\n",
    "\n",
    "    plt.rcParams['font.family'] = ['Helvetica']\n",
    "    font1 = {'family':'helvetica',\n",
    "            'weight' : 'normal',\n",
    "            'size'   : 23,\n",
    "    }\n",
    "\n",
    "    scatter_plot = sns.scatterplot(x=x, y=y, s=70) # s是点的大小\n",
    "\n",
    "    text_str = f'correlation: {corr_coefficient:.2f}'\n",
    "    plt.text(0.05, 0.95, text_str, transform=plt.gca().transAxes,\n",
    "            fontdict=font1, verticalalignment='top',)\n",
    "\n",
    "    # 添加标题和标签\n",
    "\n",
    "    plt.xlabel(l, fontdict=font1, fontsize = 27)\n",
    "    plt.ylabel(\"'programming' importance\", fontdict=font1, fontsize = 27)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    #plt.legend()\n",
    "    plt.rcParams['font.weight'] = 'bold'\n",
    "    plt.rcParams['axes.labelweight'] = 'bold'\n",
    "    ax=plt.gca();#获得坐标轴的句柄\n",
    "    ax.spines['bottom'].set_linewidth(2);###设置底部坐标轴的粗细\n",
    "    ax.spines['left'].set_linewidth(2);####设置左边坐标轴的粗细\n",
    "    ax.spines['right'].set_linewidth(2);###设置右边坐标轴的粗细\n",
    "    ax.spines['top'].set_linewidth(2);####设置上部坐标轴的粗细\n",
    "    # 显示图像\n",
    "    plt.xticks([i*0.1 for i in range(9)],[round(i*0.1,1) for i in range(9)])\n",
    "    plt.yticks([i*0.004 for i in range(5)],[i*0.004 for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f61ee5-fa24-44af-b501-43374695394e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d92ddb-3c9d-4883-a6ed-d30ccbb048a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
